---
title: "CPLN 675 Final Project - Urban Growth Modeling for Denver, 2023"
author: "Jie Li & Yinan Li"
date: "5/8/2023"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{=html}
<style>
  .superbigimage{
      overflow-x:scroll;
      white-space: nowrap;
  }

  .superbigimage img{
     max-width: none;
  }


</style>
```

# 1. Setup

Below we load the libraries needed for the analysis as well as a `mapTheme` and `plotTheme`. A set of palette colors are also specified.

```{r load_packages, message=FALSE, warning=FALSE, results = "hide"}
library(tidyverse)
library(sf)
library(raster)
library(knitr)
library(kableExtra)
library(tidycensus)
library(tigris)
library(FNN)
#library(QuantPsyc) # JE Note: in R 4.1, QuantPsyc package not available.
library(caret)
library(yardstick)
library(pscl)
library(plotROC) 
library(ggrepel)
library(pROC)
library(grid)
library(gridExtra)
library(viridis)
library(igraph)

plotTheme <- theme(
  plot.title =element_text(size=12),
  plot.subtitle = element_text(size=8),
  plot.caption = element_text(size = 6),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
  axis.text.y = element_text(size = 10),
  axis.title.y = element_text(size = 10),
  # Set the entire chart region to blank
  panel.background=element_blank(),
  plot.background=element_blank(),
  #panel.border=element_rect(colour="#F0F0F0"),
  # Format the grid
  panel.grid.major=element_line(colour="#D0D0D0",size=.75),
  axis.ticks=element_blank())

mapTheme <- theme(plot.title =element_text(size=12),
                  plot.subtitle = element_text(size=8),
                  plot.caption = element_text(size = 6),
                  axis.line=element_blank(),
                  axis.text.x=element_blank(),
                  axis.text.y=element_blank(),
                  axis.ticks=element_blank(),
                  axis.title.x=element_blank(),
                  axis.title.y=element_blank(),
                  panel.background=element_blank(),
                  panel.border=element_blank(),
                  panel.grid.major=element_line(colour = 'transparent'),
                  panel.grid.minor=element_blank(),
                  legend.direction = "vertical", 
                  legend.position = "right",
                  plot.margin = margin(1, 1, 1, 1, 'cm'),
                  legend.key.height = unit(1, "cm"), legend.key.width = unit(0.2, "cm"))

palette2 <- c("#41b6c4","#253494")
palette4 <- c("#a1dab4","#41b6c4","#2c7fb8","#253494")
palette5 <- c("#ffffcc","#a1dab4","#41b6c4","#2c7fb8","#253494")
palette10 <- c("#f7fcf0","#e0f3db","#ccebc5","#a8ddb5","#7bccc4",
               "#4eb3d3","#2b8cbe","#0868ac","#084081","#f7fcf0")
```

We also include several helper functions. `quintilesBreaks` takes a dataframe and a column and outputs the quintiles breaks, helping shorten the below `ggplot` calls.

It takes longer to `ggplot` a polygon fishnet with `geom_sf` than it does to plot `geom_point`. To cut down on plotting time, the `xyC` (for 'XY Coordinates') takes a fishnet `sf` and converts it to a dataframe of grid cell centroid coordinates.

`rast` is a function allowing us to quickly plot raster values in `ggplot`.

```{r, warning = FALSE, message = FALSE}
#this function converts a column in to quintiles. It is used for mapping.
quintileBreaks <- function(df,variable) {
    as.character(quantile(df[[variable]],
                          c(.01,.2,.4,.6,.8),na.rm=T))
}

#This function can be used to convert a polygon sf to centroids xy coords.
xyC <- function(aPolygonSF) {
  as.data.frame(
    cbind(x=st_coordinates(st_centroid(aPolygonSF))[,1],
          y=st_coordinates(st_centroid(aPolygonSF))[,2]))
} 

#this function convert a raster to a data frame so it can be plotted in ggplot
rast <- function(inRaster) {
  data.frame(
    xyFromCell(inRaster, 1:ncell(inRaster)), 
    value = getValues(inRaster)) }
```

# 2. Demand-side Change Scenario Prediction

In this section a considerable amount of vector and raster data is wrangled together into a regression-ready dataset. The following datasets are used:

2.1: Land cover change data [downloaded](https://www.mrlc.gov/data/nlcd-land-cover-change-index-conus) from the Multi-Resolution Land Characteristics Consortium's National Land Cover Database (NLCD) includes annual land cover and land cover change raster data for the entire country. These data are sampled to a 4,000 by 4,000 ft\^2 fishnet, which will be used for.

2.2: The land cover data in 2009 is shown in this part

2.3: Population data is downloaded from the U.S. Census and joined to the fishnet by distributing Census Tract population totals proportionally to each grid cell.

2.4: The land cover change data is used to engineer spatial lag features.

2.5: County polygons are downloaded using the `tigris` package.

2.6: Each feature is wrangled into a final dataset.

2.7: In this section we explore the extent to which each features is associated with development change.

2.8: Five models are trained to predict the 2029 land cover demand.

## 2.1. Land Cover Change Data

The dependent variable we wish to forecast is land cover change between 2009 and 2019. In this section, the land cover raster data is loaded, reclassified and integrated with a vector fishnet. As before, the fishnet will allow us to parametrize spatial relationships in a regression context.

The table below shows descriptions of each categorical land cover type in the land cover data. Below, we will reclassify these data into more useful categories.

The land cover categories are referred to the National Land Cover Database Class Legend and Description [here](https://www.mrlc.gov/data/legends/national-land-cover-database-class-legend-and-description)

Several raster layers have been provided for this analysis:

-   We read in `denverMSA` (2019) - this is the extent of the study area

-   We read in `denv_lc_2009.tif` to represent the land cover data of 2009 and `den_lc_2019.tif` as the most recent land cover data. Then we use these two land cover raster to calculate the land cover change in the decade, which is `lc_change`

-   `lc_change` is a raster of land cover change - where there were conversions between one land cover and another on the time frame 2009-2019. We plot the raster using `ggplot` and the `rast` function specified above. The original land cover raster is at a 30 meter by 30 meter resolution. To improve the efficiency of calculation, rasters provided are ultimately resampled up to 4000 feet by 4000 feet.

Note that both `denverMSA` and `lc_change` are projected as `NAD 1983 HARN StatePlane Colorado Central FIPS 0502 (US Feet)`.

```{r load_data, warning = FALSE, message = FALSE, results = "hide"}
denverMSA <- 
  st_read("https://raw.githubusercontent.com/YinanLi-15/CPLN675/main/denverMSA.json") 

lc_2019 <- raster("https://raw.githubusercontent.com/YinanLi-15/CPLN675/main/den_lc_2019.tif")
lc_2009 <- raster("https://raw.githubusercontent.com/YinanLi-15/CPLN675/main/den_lc_2009.tif")

lc_change <- lc_2009 - lc_2019
```

We now plot the MSA.

```{r plot_msa, warning= FALSE, message= FALSE}
ggplot() +
  geom_sf(data=denverMSA) +
  geom_raster(data=rast(lc_change) %>% na.omit %>% filter(value > 0), 
              aes(x,y,fill=as.factor(value))) +
  scale_fill_viridis(direction = -1, discrete=TRUE, name ="Land Cover\nChange") +
  labs(title = "Land Cover Change, 2009-2019") +
  mapTheme +
  theme(legend.direction="horizontal")

```

Next, we reclassify the raster such that all the developed grid cell values receive a value of 1 and all other values receive a value of 0. This is done using a reclassify matrix. The matrix reads row by row. Row 1 says any grid cell ranging from 0 to 12 takes a value of 0; 13 or greater through 24, a value of 1; and all other values take 0.

```{r, warning = FALSE, message = FALSE}
reclassMatrix <- 
  matrix(c(
    0,12,0,
    12,24,1,
    24,Inf,0),
  ncol=3, byrow=T)

reclassMatrix
```

Now `reclassify` and convert all 0's to `NA`. We apply a name to the raster with `names`. This is done to make it faster to join raster to the fishnet below. You can see the frequency table of values with `freq(lc_change2)`. 

```{r, warning = FALSE, message = FALSE}
lc_change2 <- 
  reclassify(lc_change,reclassMatrix)

lc_change2[lc_change2 < 1] <- NA

names(lc_change2) <- "lc_change"

ggplot() +
  geom_sf(data=denverMSA) +
  geom_raster(data=rast(lc_change2) %>% na.omit, 
              aes(x,y,fill=as.factor(value))) +
  scale_fill_viridis(discrete=TRUE, name ="Land Cover\nChange") + 
  labs(title="Development Land Use Change") +
  mapTheme
```

Next, the fishnet is created at 4000 by 4000 feet resolution and subset it to the denver MSA with `st_intersection`.

```{r, warning = FALSE, message = FALSE}
denverMSA_fishnet <- 
  st_make_grid(denverMSA, 4000) %>%
  st_sf()

denverMSA_fishnet <-
  denverMSA_fishnet[denverMSA,]
```

The vector fishnet is then plotted.

```{r, warning = FALSE, message= FALSE}
ggplot() +
  geom_sf(data=denverMSA_fishnet) +
  labs(title="Fishnet, 4000 Feet Resolution") +
  mapTheme
```

Then the raster is converted to points, which makes its joining to the vector fishnet a bit faster. Now to extract the raster values into the fishnet. There is a function in the `raster` package called `RasterToPolygon` but it is quite slow.

To speed up the mapping process, fishnet polygons are converted to centroid points using the `xyC` function defined

```{r, warning = FALSE, message = FALSE}
changePoints <-
  rasterToPoints(lc_change2) %>%
  as.data.frame() %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(denverMSA_fishnet))

fishnet <- 
  aggregate(changePoints, denverMSA_fishnet, sum) %>%
  mutate(lc_change = ifelse(is.na(lc_change),0,1),
         lc_change = as.factor(lc_change))

ggplot() +
  geom_sf(data=denverMSA) +
  geom_point(data=fishnet, 
             aes(x=xyC(fishnet)$x, y=xyC(fishnet)$y, colour=lc_change)) +
  scale_colour_manual(values = palette2,
                      labels=c("No Change","New Development"),
                      name = "") +
  labs(title = "Land Cover Development Change", subtitle = "As fishnet centroids") +
  mapTheme
```

## 2.2. Land Cover in 2009

It is reasonable to hypothesize that the propensity of new development is a function of existing land cover categories. In this section we identify these other land cover categories from 2009 and integrate each with the fishnet.

```{r, warning = FALSE, message = FALSE}
ggplot() +
  geom_sf(data=denverMSA) +
  geom_raster(data=rast(lc_2009) %>% na.omit %>% filter(value > 0), 
              aes(x,y,fill=as.factor(value))) +
  scale_fill_viridis(discrete=TRUE, name ="") +
  labs(title = "Land Cover, 2009") +
  mapTheme +
  theme(legend.direction="horizontal")
```

The table below shows the approach taken to recoded existing land cover codes into the categories used in our analysis. In the code block below new rasters are generated and `names` are applied. Naming ensures that when the raster is integrated with the fishnet, the column reflects the appropriate raster.

| Old_Classification                                               | New_Classification |
|------------------------------------------------------|------------------|
| Open Space as well as Low, Medium and High Intensity Development | Developed          |
| Deciduous, Evergreen, and Mixed Forest                           | Forest             |
| Pasture/Hay and Cultivated Crops                                 | Farm               |
| Woody and Emergent Herbaceous Wetlands                           | Woodlands          |
| Barren Land, Dwarf Scrub, and Grassland/Herbaceous               | Other Undeveloped  |
| Water                                                            | Water              |

```{r, warning = FALSE, message = FALSE}
developed <- lc_2009 == 21 | lc_2009 == 22 | lc_2009 == 23 | lc_2009 == 24
forest <- lc_2009 == 41 | lc_2009 == 42 | lc_2009 == 43 
farm <- lc_2009 == 81 | lc_2009 == 82 
wetlands <- lc_2009 == 90 | lc_2009 == 95 
otherUndeveloped <- lc_2009 == 52 | lc_2009 == 71 | lc_2009 == 31 
water <- lc_2009 == 11

names(developed) <- "developed"
names(forest) <- "forest"
names(farm) <- "farm"
names(wetlands) <- "wetlands"
names(otherUndeveloped) <- "otherUndeveloped"
names(water) <- "water"
```

Next, each raster is aggregated to the fishnet by way of a function called `aggregateRaster`. Here, the process used above to do this, a function is created below that loops through a list of rasters, converts the *ith* raster to points, filters only points that have value of `1` (ie. is the *ith* land cover type), and then aggregates to the fishnet.

```{r, warning = FALSE, message = FALSE}
aggregateRaster <- function(inputRasterList, theFishnet) {
  #create an empty fishnet with the same dimensions as the input fishnet
  theseFishnets <- theFishnet %>% dplyr::select()
  #for each raster in the raster list
  for (i in inputRasterList) {
  #create a variable name corresponding to the ith raster
  varName <- names(i)
  #convert raster to points as an sf
    thesePoints <-
      rasterToPoints(i) %>%
      as.data.frame() %>%
      st_as_sf(coords = c("x", "y"), crs = st_crs(theFishnet)) %>%
      filter(.[[1]] == 1)
  #aggregate to the fishnet
    thisFishnet <-
      aggregate(thesePoints, theFishnet, length) %>%
      mutate(!!varName := ifelse(is.na(.[[1]]),0,1))
  #add to the larger fishnet
    theseFishnets <- cbind(theseFishnets,thisFishnet)
  }
  #output all aggregates as one large fishnet
   return(theseFishnets)
  }
```

The `theRasterList` of land cover types in 2009 is created and then fed into `aggregateRaster`. The result is converted to long form grid cell centroids and plot as small multiple maps.

```{r, warning = FALSE, message = FALSE}
theRasterList <- c(developed,forest,farm,wetlands,otherUndeveloped,water)

aggregatedRasters <-
  aggregateRaster(theRasterList, denverMSA_fishnet) %>%
  dplyr::select(developed,forest,farm,wetlands,otherUndeveloped,water) %>%
  mutate_if(is.numeric,as.factor)

aggregatedRasters %>%
  gather(var,value,developed:water) %>%
  st_cast("POLYGON") %>%    #just to make sure no weird geometries slipped in
  mutate(X = xyC(.)$x,
         Y = xyC(.)$y) %>%
  ggplot() +
    geom_sf(data=denverMSA) +
    geom_point(aes(X,Y, colour=as.factor(value))) +
    facet_wrap(~var) +
    scale_colour_manual(values = palette2,
                        labels=c("Other","Land Cover"),
                        name = "") +
    labs(title = "Land Cover Types, 2009",
         subtitle = "As fishnet centroids") +
   mapTheme
```

## 2.3. Census Data

Population and population change is obviously an critical demand-side component of predicting `Development_Demand`. Census data for both 2009 and 2019 can be downloaded quickly using the `tidycensus` package. As illustrated below, these data are downloaded at a census tract geography and thus, an approach is needed to reconcile tracts and fishnet geometries. This is accomplished using a technique called areal weighted interpolation.

```{r, warning = FALSE, message = FALSE, results = "hide"}
# to see the variables in the dataset
load_variables(year = 2009, dataset = "acs5", cache = TRUE)
```

```{r, warning = FALSE, message = FALSE, results = "hide"}
denverPop09 <- 
  get_acs(geography = "tract", variables = "B00001_001", year = 2009,
          state = 8, geometry = TRUE, 
          county=c("Adams","Arapahoe","Broomfield", "Clear Creek", "Denver","Douglas", "Elbert", "Gilpin", "Jefferson", "Park"),
          key = "d7ba584881403bf189db041a5a8f87dafdc76f81") %>%
  rename(pop_2009 = estimate) %>%
  st_transform(st_crs(denverMSA_fishnet))
```

Now data for 2019 is downloaded. In this instance, `st_buffer` is used to buffer the tracts by -1ft. This is done because `tidycensus` appears to return geometries that are problematic when subjected to the area weighted interpolation function below. As done in previous chapters, a very small buffer is used to correct the geometries.

```{r, warning = FALSE, message = FALSE, results = "hide"}
# to see the variables in the dataset
load_variables(year = 2019, dataset = "acs5", cache = TRUE)
```

```{r, warning = FALSE, message = FALSE, results = "hide"}
denverPop19 <- 
  get_acs(geography = "tract", variables = "B01003_001", year = 2019,
          state = 8, geometry = TRUE, 
          county=c("Adams","Arapahoe","Broomfield", "Clear Creek", "Denver","Douglas", "Elbert", "Gilpin", "Jefferson", "Park"),
          key = "d7ba584881403bf189db041a5a8f87dafdc76f81") %>%
  rename(pop_2019 = estimate) %>%
  st_transform(st_crs(denverMSA_fishnet)) %>%
  st_buffer(-1)
```

Both years of census data are then plotted.

::: superbigimage
```{r, warning = FALSE, message = FALSE, fig.height= 8, fig.width= 11}
grid.arrange(
ggplot() +
  geom_sf(data = denverPop09, aes(fill=factor(ntile(pop_2009,5))), colour=NA) +
  scale_fill_manual(values = palette5,
                    labels=quintileBreaks(denverPop09,"pop_2009"),
                   name="Quintile\nBreaks") +
  labs(title="Population, Denver MSA: 2009") +
  mapTheme,

ggplot() +
  geom_sf(data = denverPop19, aes(fill=factor(ntile(pop_2019,5))), colour=NA) +
  scale_fill_manual(values = palette5,
                    labels=quintileBreaks(denverPop19,"pop_2019"),
                   name="Quintile\nBreaks") +
  labs(title="Population, Denver MSA: 2019") +
  mapTheme, ncol=2)
```
:::

Now to reconcile tract boundaries and fishnet grid cells.

A spatial join would be inappropriate as it would assign the same population value from one tract to the many intersecting grid cells. Instead, the area weighted interpolation function, `st_interpolate_aw`, assigns a proportion of a tract's population to a grid cell weighted by the proportion of the tract that intersects the grid cell. This works best of course, when we assume that the tract population is uniformly distributed across the tract. This is typically not a great assumption. However, it is a reasonable here particularly given population is a feature in a regression and not an outcome that needs to be measured with significant precision.

The Census data `denverPop`, has a different spatial extent than `denverMSA_fishnet`. Most notably, there are no vectors where water is present. To maintain the needed grid cells units `(nrow(denverMSA_fishnet))`, a unique id is created, `fishnetID`. Then the area weighted interpolation is performed on `population` for the 2009 and 2019 layers. Finally, the results are joined back (`left_join`) to `denverMSA_fishnet`. This approach maintains a consistent spatial extent.

```{r, warning = FALSE, message = FALSE}
denverMSA_fishnet <-
  denverMSA_fishnet %>%
  rownames_to_column("fishnetID") %>% 
  mutate(fishnetID = as.numeric(fishnetID)) %>%
  dplyr::select(fishnetID)

fishnetPopulation09 <-
  st_interpolate_aw(denverPop09["pop_2009"], denverMSA_fishnet, extensive=TRUE) %>%
  as.data.frame(.) %>%
  rownames_to_column(var = "fishnetID") %>%
  left_join(denverMSA_fishnet %>%
              mutate(fishnetID = as.character(fishnetID)),
            ., by=c("fishnetID"='fishnetID')) %>% 
  mutate(pop_2009 = replace_na(pop_2009,0)) %>%
  dplyr::select(pop_2009)

fishnetPopulation19 <-
  st_interpolate_aw(denverPop19["pop_2019"],denverMSA_fishnet, extensive=TRUE) %>%
  as.data.frame(.) %>%
  rownames_to_column(var = "fishnetID") %>%
  left_join(denverMSA_fishnet %>%
              mutate(fishnetID = as.character(fishnetID)),
            ., by=c("fishnetID"='fishnetID')) %>% 
  mutate(pop_2019 = replace_na(pop_2019,0)) %>%
  dplyr::select(pop_2019)

fishnetPopulation <- 
  cbind(fishnetPopulation09,fishnetPopulation19) %>%
  dplyr::select(pop_2009,pop_2019) %>%
  mutate(pop_Change = pop_2019 - pop_2009)
```

For comparison purposes, both the 2019 census tract geometries and the population weighted grid cells are plotted.

::: superbigimage
```{r, warning = FALSE, message = FALSE, fig.height = 8, fig.width= 11}
grid.arrange(
ggplot() +
  geom_sf(data=denverPop19, aes(fill=factor(ntile(pop_2019,5))),colour=NA) +
  scale_fill_manual(values = palette5,
                    labels=substr(quintileBreaks(denverPop19,"pop_2019"),1,4),
                   name="Quintile\nBreaks") +
  labs(title="Population, denver MSA: 2019",
       subtitle="Represented as tracts; Boundaries omitted") +
  mapTheme,

ggplot() +
  geom_sf(data=fishnetPopulation, aes(fill=factor(ntile(pop_2019,5))),colour=NA) +
  scale_fill_manual(values = palette5,
                   labels=substr(quintileBreaks(fishnetPopulation,"pop_2019"),1,4),
                   name="Quintile\nBreaks") +
  labs(title="Population, denver MSA: 2019",
       subtitle="Represented as fishnet gridcells; Boundaries omitted") +
  mapTheme, ncol=2)
```
:::

## 2.4. The Spatial Lag of Development

In Denver, as in many sprawling regions of the U.S. the economic incentives that underlie sprawl likely encourage both the accessibility and leapfrog models of development. For our purposes however, features must be created to associate these patterns with development. Without them, the model may lack the appropriate spatial experience on which to forecast growth.

To keep it simple, we develop features associated with accessibility-based patterns. In reality, the analyst should develop a series of applicable features and test which best associate with the outcome of interest. The problem becomes infinitely more difficult when one realizes that sprawl patterns may differ throughout the study area - if for instance, land use restrictions varied by county. Below we estimate models using logistic regression, but higher level machine learning algorithms, most notably, Random Forest, are more adept at dealing with non-linearities across space.

Accessibility is measured by way of a spatial lag hypothesizing that new development is a function of distance to existing development. The shorter the distance, the more accessible a grid cell is to existing development. This is measured by calculating the average distance from each grid cell to its 2 nearest developed neighboring grid cells in 2001 using the `nn_function`. The function below calculates average nearest neighbor distance between k point layers. The first parameter specifies coordinates that we want to `measureFrom`, in this case, `fishnet` centroids. The second, indicates the point layer we wish to `measureTo`, in this case, the fishnet centroids that were developed in 2009.

```{r, warning = FALSE, message = FALSE}
nn_function <- function(measureFrom,measureTo,k) {
  #convert the sf layers to matrices
  measureFrom_Matrix <-
    as.matrix(measureFrom)
  measureTo_Matrix <-
    as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
    output <-
    as.data.frame(nn) %>%
    rownames_to_column(var = "thisPoint") %>%
    gather(points, point_distance, V1:ncol(.)) %>%
    arrange(as.numeric(thisPoint)) %>%
    group_by(thisPoint) %>%
    summarize(pointDistance = mean(point_distance)) %>%
    arrange(as.numeric(thisPoint)) %>% 
    dplyr::select(-thisPoint) %>%
    pull()
  
  return(output)  
}
```

Next, the function appending the lag distance to `fishnet`. There are 3 inputs. The `fishnet` which is converted to a coordinate data frame with the `xyC` function. 2001 developed areas are created using `filter`. The map below illustrates relative accessibility from every grid cell to nearby development.

```{r, warning = FALSE, message = FALSE}
fishnet$lagDevelopment <-
    nn_function(xyC(fishnet),
                xyC(filter(aggregatedRasters,developed==1)),
                2)

ggplot() +
  geom_sf(data=denverMSA) +
  geom_point(data=fishnet, 
             aes(x=xyC(fishnet)[,1], y=xyC(fishnet)[,2], 
                 colour=factor(ntile(lagDevelopment,5))), size=1.5) +
  scale_colour_manual(values = palette5,
                     labels=substr(quintileBreaks(fishnet,"lagDevelopment"),1,7),
                     name="Quintile\nBreaks") +
  labs(title = "Spatial Lag to 2009 Development",
       subtitle = "As fishnet centroids") +
  mapTheme
```

## 2.5. MSA Counties

The `tigris` package allows Texas county geometries to be downloaded. A spatial subset returns only the counties in the MSA. Note that the subset includes a negative 1000ft `st_buffer`. This is done because the spatial extent of `denverMSA` intersects county boundaries that are actually outside of our study area. Buffering `denverMSA` slightly limits the intersection range to only those counties in the study area.

Once `studyAreaCounties` is created, it is `st_join`ed with `dat` such that each grid cells knows which county it's in.

```{r, warning = FALSE, message = FALSE, results = "hide"}
options(tigris_class = "sf")

studyAreaCounties <- 
  counties("Colorado") %>%
  st_transform(st_crs(denverMSA)) %>%
  dplyr::select(NAME) %>%
  .[st_buffer(denverMSA,-1000), , op=st_intersects]
```

```{r, warning = FALSE, message = FALSE}
ggplot() +
  geom_sf(data=studyAreaCounties) +
  labs(title = "Study Area Counties") +
  mapTheme
```

## 2.6. Create the Final Dataset

The last step is to bring together all the disparate feature layers into a final dataset that can be used for analysis. The various fishnet layers are `cbind` together, needed features are extracted and the final fishnet, `dat` is then joined with `studyAreaCounties` to assign each grid cell to a county. `developed19` is created to designate those areas that have already been developed through 2019. Finally, any grid cell that has a `water` land cover designation is removed.

```{r, warning = FALSE, message = FALSE}
dat <- 
  cbind(
    fishnet, fishnetPopulation, aggregatedRasters) %>%
  dplyr::select(lc_change, developed, forest, farm, wetlands, otherUndeveloped, water,
                pop_2009, pop_2019, pop_Change, lagDevelopment) %>%
  st_join(studyAreaCounties) %>%
  mutate(developed10 = ifelse(lc_change == 1 & developed == 1, 0, developed)) %>%
  filter(water == 0) 
```

## 2.7. Exploratory Analysis

If the goal was to predict a continuous variable, scatterplots and correlation coefficients make this process straightforward and relatively easy to explain to a non-technical decison maker.

In this case however, the dependent variable is a binary outcome - either a grid cell was developed between 2009 and 2019 or it wasn't. In this case, the relevant question is whether for a given feature, there is a statistically significant difference between areas that changed and areas that did not. These differences are explored in a set of plots below. For models with lots of features, these plots could be compliment by a series of difference in means statistical tests.

```{r, warning = FALSE, message = FALSE}
dat %>%
  dplyr::select(lagDevelopment,lc_change) %>%
  gather(Variable, Value, -lc_change, -geometry) %>%
  ggplot(., aes(lc_change, Value, fill=lc_change)) + 
    geom_bar(position = "dodge", stat = "summary", fun.y = "mean") +
    facet_wrap(~Variable) +
    scale_fill_manual(values = palette2,
                      labels=c("No Change","New Development"),
                      name="") +
    labs(title="New Development as a Function of the Continuous Variables") +
    plotTheme 
```

Next, the same visualization is created for the population related variables.

```{r, warning = FALSE, message = FALSE}
dat %>%
  dplyr::select(pop_2009,pop_2019,pop_Change,lc_change) %>%
  gather(Variable, Value, -lc_change, -geometry) %>%
  ggplot(., aes(lc_change, Value, fill=lc_change)) + 
    geom_bar(position = "dodge", stat = "summary", fun.y = "mean") +
    facet_wrap(~Variable) +
    scale_fill_manual(values = palette2,
                      labels=c("No Change","New Development"),
                      name="") +
    labs(title="New Development as a Function of Factor Variables") +
    plotTheme
```

Next, a table of land cover conversion between 2009 and 2019 is created. The table suggests for instance, that 1% of farmland regionally was converted to development between 2009 and 2019. This indicator should be interpreted in the context of the scale changes we imposed on the data by moving from a 30ft by 30ft raster to a 4000ft by 4000ft fishnet. This is the same reason why the table suggests `developed` area was then "developed".

```{r, warning = FALSE, message = FALSE}
dat %>%
  dplyr::select(lc_change:otherUndeveloped,developed) %>%
  gather(Land_Cover_Type, Value, -lc_change, -geometry) %>%
   st_set_geometry(NULL) %>%
     group_by(lc_change, Land_Cover_Type) %>%
     summarize(n = sum(as.numeric(Value))) %>%
     ungroup() %>%
    mutate(Conversion_Rate = paste0(round(100 * n/sum(n), 2), "%")) %>%
    filter(lc_change == 1) %>%
  dplyr::select(Land_Cover_Type,Conversion_Rate) %>%
  kable() %>% kable_styling(full_width = F)
```

## 2.8. Predicting for 2029

In this section, five separate logistic regression models are estimated to predict development change between 2009 and 2019 - with each subsequent model more sophisticated then the last. To do so, the data is split into 50% training/test sets. Models are estimated on the training set.

### 2.8.1. Modeling

First, `dat` is split into training and test sets. Note how imbalanced the panel is with `table(datTrain$lc_change1)`.

```{r, warning = FALSE, message = FALSE}
set.seed(3456)
trainIndex <- 
  createDataPartition(dat$developed, p = .50,
                                  list = FALSE,
                                  times = 1)
datTrain <- dat[ trainIndex,]
datTest  <- dat[-trainIndex,]

nrow(dat)
```

Next six separate `glm` models are estimated adding new variables for each. Figure 4.1 shows the Psuedo R-Squared associated with each model.

`Model1` includes only the 2009 land cover types. `Model2` adds the `lagDevelopment`. Models 3, 4 and 5 attempt three different approaches for modeling population change. `Model3` uses population in 2009; `Model4` uses 2009 and 2019 population; and `Model5` uses population change. 

```{r, warning = FALSE, message = FALSE}
Model1 <- glm(lc_change ~ wetlands + forest  + farm + otherUndeveloped, 
              family="binomial"(link="logit"), data = datTrain)

Model2 <- glm(lc_change ~ wetlands + forest  + farm + otherUndeveloped + lagDevelopment, 
              family="binomial"(link="logit"), data = datTrain)
              
Model3 <- glm(lc_change ~ wetlands + forest  + farm + otherUndeveloped + lagDevelopment + pop_2009, 
              family="binomial"(link="logit"), data = datTrain)          
              
Model4 <- glm(lc_change ~ wetlands + forest  + farm + otherUndeveloped + lagDevelopment + pop_2009 + 
              pop_2019, 
              family="binomial"(link="logit"), data = datTrain)              
            
Model5 <- glm(lc_change ~ wetlands + forest  + farm + otherUndeveloped + lagDevelopment + pop_Change, 
              family="binomial"(link="logit"), data = datTrain)              
```

Working carefully through the below code block, a very concise approach for creating a data frame of psudeo R Squares for each model and plotting them for comparison. Recall, `pR2` is the function for psuedo R squared. Dissect the line that uses the `map_dfc` function to see how this approach loops through the models retrieving the goodness of fit for each.

```{r, warning = FALSE, message = FALSE}
modelList <- paste0("Model", 1:5)
map_dfc(modelList, function(x)pR2(get(x)))[4,] %>%
  setNames(paste0("Model",1:5)) %>%
  gather(Model,McFadden) %>%
  ggplot(aes(Model,McFadden)) +
    geom_bar(stat="identity") +
    labs(title= "McFadden R-Squared by Model") +
    plotTheme
```

Next, a data frame is created that includes columns for the observed development change, `lc_change`, and one that includes predicted probabilities for `Model4`. This data frame is then used as an input to a density plot visualizing the distribution of predicted probabilities by observed class. Only a small number of predicted probabilities are greater than or equal to 50% `(nrow(filter(testSetProbs, probs >= .50)) / nrow(datTest))`. 

```{r, warning = FALSE, message = FALSE}
testSetProbs <- 
  data.frame(class = datTest$lc_change,
             probs = predict(Model4, datTest, type="response")) 
  
ggplot(testSetProbs, aes(probs)) +
  geom_density(aes(fill=class), alpha=0.5) +
  scale_fill_manual(values = palette2,
                    labels=c("No Change","New Development")) +
  labs(title = "Histogram of test set predicted probabilities",
       x="Predicted Probabilities",y="Density") +
  plotTheme
```

### 2.8.2. Accuracy

Now to pick a predicted probability threshold to classify an area as having new development. Recall, *Sensitivity* or the True Positive rate is the proportion of actual positives (1's) that were predicted to be positive. For example, the Sensitivity in our model is the rate of developed areas actually predicted as such. *Specificity* or True Negative Rate is the proportion of actual negatives (0's) that were predicted to be negatives. For example, the Specificity in our model is the rate of No Change areas that were correctly predicted as No change.

It is important to consider what Planners would typically optimize for given this use case. One approach is to maximize the number of 1's predicted correctly (Sensitivity) so as to not under or over-predict new development. It may okay in this use case to incorrectly predict no change as changed (Specificity). An abundance of False Negative errors may be reasonable if Planners don't mind over emphasizing development potential. It is important to remember that below this potential will evaluated alongside supply-side indicators such as the presence of sensitive land.

There are some clear tradeoffs between Sensitivity and Specificity in our model that deserve some exploration. To illustrate, two different thresholds of 5% and 17% are explored. Predicted classes for both thresholds are generated and instead of using the `confusionMatrix` function from `caret` as we have in the past, here confusion matrix metrics are derived from the `yardstick` package. This allows us to `group_by` the threshold and `summarize` the metrics of interest.

The `options` call below is required to tell `yardstick` that the positive factor class in `testSetProbs` is `1`. Without it, yardstick will by default, see the first factor level as `0` and flip the confusion metrics around.

```{r, warning = FALSE, message = FALSE}
options(yardstick.event_first = FALSE)

testSetProbs <- 
  testSetProbs %>% 
  mutate(predClass_05 = as.factor(ifelse(testSetProbs$probs >= 0.05 ,1,0)),
         predClass_17 = as.factor(ifelse(testSetProbs$probs >= 0.17 ,1,0))) 

testSetProbs %>%
  dplyr::select(-probs) %>%
  gather(Variable, Value, -class) %>%
  group_by(Variable) %>%
  summarize(Value = factor(Value, levels = levels(class)),
            Sensitivity = round(yardstick::sens_vec(class, Value), 2),
            Specificity = round(yardstick::spec_vec(class, Value), 2),
            Accuracy = round(yardstick::accuracy_vec(class, Value), 2)) %>%
  kable() %>%
  kable_styling(full_width = F)
```

The 5% threshold correctly predicts a higher number of new development areas (Sensitivity), but incorrectly predicts a lower number of no change areas (Specificity). As there are far more no change areas in the data, this is reflected in a lower overall accuracy. Conversely, the 17% threshold has a lower Sensitivity rate and but a far higher Specificity rate. Again, because of the dataset is majority no change areas, this leads to a far higher Accuracy rate.

Given the use case, and the spatial distribution of land cover change, it may be more useful to have a model that predicts generally where new development occurs rather than one that predicts precisely where. As illustrated below, the 17% threshold provides this outcome. These trade-offs can be visualized in the plot below. Here the model is used to predict for the entire `dat` dataset. Which threshold looks more reasonable given the distribution of observed development change?

Note that these indicators are converted `as.factor` so they can be mapped with `scale_color_manual`.

```{r, warning = FALSE, message = FALSE}
predsForMap <-         
  dat %>%
    mutate(probs = predict(Model4, dat, type="response") ,
           Threshold_5_Pct = as.factor(ifelse(probs >= 0.05 ,1,0)),
           Threshold_17_Pct =  as.factor(ifelse(probs >= 0.17 ,1,0))) %>%
    dplyr::select(lc_change,Threshold_5_Pct,Threshold_17_Pct) %>%
    gather(Variable,Value, -geometry) %>%
    st_cast("POLYGON")
```

::: superbigimage
```{r, warning = FALSE, message= FALSE, fig.height = 3, fig.width= 8}
ggplot() +
  geom_point(data=predsForMap, aes(x=xyC(predsForMap)[,1], y=xyC(predsForMap)[,2], colour=Value)) +
  facet_wrap(~Variable) +
  scale_colour_manual(values = palette2, labels=c("No Change","New Development"),
                      name="") +
  labs(title="Development Predictions - Low Threshold") + 
  mapTheme
```
:::

To provide a bit more insight, the code block below produces both true positives (Sensitivity) and true negatives (Specificity) for each grid cell by threshold type. Notice how the spatial pattern of Sensitivity for both thresholds is relatively consistent, but the 5% threshold misses most the study area with respect to Specificity.

```{r, warning = FALSE, message = FALSE}
ConfusionMatrix.metrics <-
  dat %>%
    mutate(probs = predict(Model4, dat, type="response") ,
           Threshold_5_Pct = as.factor(ifelse(probs >= 0.05 ,1,0)),
           Threshold_17_Pct =  as.factor(ifelse(probs >= 0.17 ,1,0))) %>%
    mutate(TrueP_05 = ifelse(lc_change  == 1 & Threshold_5_Pct == 1, 1,0),
           TrueN_05 = ifelse(lc_change  == 0 & Threshold_5_Pct == 0, 1,0),
           TrueP_17 = ifelse(lc_change  == 1 & Threshold_17_Pct == 1, 1,0),
           TrueN_17 = ifelse(lc_change  == 0 & Threshold_17_Pct == 0, 1,0)) %>%
    dplyr::select(., starts_with("True")) %>%
    gather(Variable, Value, -geometry) %>%
    st_cast("POLYGON") 
```

::: superbigimage
```{r, warning = FALSE, message = FALSE, fig.height= 6, fig.width= 8 }
ggplot(data=ConfusionMatrix.metrics) +
  geom_point(aes(x=xyC(ConfusionMatrix.metrics)[,1], 
                 y=xyC(ConfusionMatrix.metrics)[,2], colour = as.factor(Value))) +
  facet_wrap(~Variable) +
  scale_colour_manual(values = palette2, labels=c("Correct","Incorrect"),
                       name="") +
  labs(title="Development Predictions - Low Threshold") + mapTheme
```
:::

### 2.8.3. Generalizability

For this use case, it matters little whether the model generalizes well across random holdouts. Thus, regular cross-validation is substituted for spatial cross-validation. The latter is explicitly concerned with generalizability across space. The approach helps us understand whether our model is comparable to each county in the study area despite any possible differences in land use or land use planning.

To test across-space generalizability, `spatialCV` function is run, which iteratively loops through `dat` having each county take a turn as the hold out test set. This is also called 'Leave-one-group-out cross validation.'. A model is estimated for the n - 1 counties that remain and used to `predict` for the hold out county.

```{r, warning = FALSE, message = FALSE}
spatialCV <- function(dataFrame, uniqueID, dependentVariable, modelName) {

#initialize a data frame 
endList <- list()

#create a list that is all the spatial group unqiue ids in the data frame (ie counties)    
  uniqueID_List <- unique(dataFrame[[uniqueID]])  
  x <- 1
  y <- length(uniqueID_List)
  
#create a counter and while it is less than the number of counties...  
  while(x <= y) 
  {
#call a current county    
    currentUniqueID <- uniqueID_List[x]
#create a training set comprised of units not in that county and a test set of units
#that are that county
    training <- dataFrame[ which(dataFrame[[uniqueID]] != uniqueID_List[x]),]
    testing <- dataFrame[ which(dataFrame[[uniqueID]] == uniqueID_List[x]),]
#create seperate xy vectors
    trainingX <- training[ , -which(names(training) %in% c(dependentVariable))]
    testingX <- testing[ , -which(names(testing) %in% c(dependentVariable))]
    
    trainY <- training[[dependentVariable]]
    testY <- testing[[dependentVariable]]
#Calculate predictions on the test county as part of a data frame including the observed
#outcome and the unique county ID    
   thisPrediction <- 
     data.frame(class = testY,
                probs = predict(modelName, testingX, type="response"),
                county = currentUniqueID) 

#Row bind the predictions to a data farme
   endList <- rbind(endList, thisPrediction)
#iterate counter    
    x <- x + 1 
  } 
#return the final list of counties and associated predictions  
  return (as.data.frame(endList))
}
```

Now the function is run; a 17% predicted probability threshhold is set and a facetted ROC plot for each county is created. It is important to note that Chambers, Austin, Waller, Liberty and San Jacinto each have ony between 1 and 13 grid cells that actually developed, so the below goodness of fit metrics are likely not useful.

```{r, warning = FALSE, message = FALSE}
dat <- na.omit(dat) # Remove rows with missing values from the original dataframe

spatialCV_counties <- spatialCV(dat, "NAME", "lc_change", Model4) %>%
  mutate(predClass = as.factor(ifelse(probs >= 0.17, 1, 0)))
```

To investigate the across-county generalizability of the model, the code block below produces and maps confusion matrix statistics by county. It is important to ensure as above, that the `yardstick.event_first` option is set.

```{r, warning = FALSE, message = FALSE}
spatialCV_metrics <- spatialCV_counties %>% 
  group_by(county) %>% 
  mutate(predClass = as.factor(ifelse(probs >= 0.17, 1, 0))) %>%
  ungroup() %>%
  mutate(class = as.factor(class)) %>%
  {levels(.$class) <- c(0, 1); levels(.$predClass) <- c(0, 1); .} %>%
  summarize(Observed_Change = sum(as.numeric(as.character(class))),
            Sensitivity = round(yardstick::sens_vec(class, predClass), 2),
            Specificity = round(yardstick::spec_vec(class, predClass), 2),
            Accuracy = round(yardstick::accuracy_vec(class, predClass), 2))
```

### 2.8.4. Predicting Land Cover Demand for 2029

At this point, a simple but useful model has been trained to predict urban development between 2009 and 2019 as a function of baseline features from 2009 including land cover, built environment and population. Next, we are going to updated our features to reflect a 2019 baseline. Having done so, predictions from our new model would then before 2029.

For brevity, we only update two features in our model. First, population change (`pop_change`) is updated using county level population projections visualized in the plot below. The second is `lagDevelopment`, which describes how predicted new development relates in space to old development.

Once the features are updated, 2029 predictions are estimated and mapped.

Below, `lagDevelopment` is mutate describing average distance to 2019 development. Note that the field name, `lagDevelopment` is unchanged (ie. not updated to `lagDevelopment_2019`). This is done purposefully as model6 has a regression coefficient called `lagDevelopment`. If this variable wasn't present in our updated data frame then the `predict` command would fail.

```{r, warning = FALSE, message = FALSE}
dat <-
  dat %>%
  mutate(lagDevelopment = nn_function(xyC(.), xyC(filter(.,developed10 == 1)),2))
```

Now to update population change. A new data frame, `countyPopulation_2029` is created which includes 2019 population counts and 2029 projections for each county in the study area. Population is plotted by year and by county. Denver's Harris County is projected to see the greatest population gains by far. Anecdotally, we know that much of Harris County is already developed, which suggests its development scenario will involve more 'infill' development then sprawl.

We use the population projection for 2030 from Colorado Department of Transportation https://dtdapps.coloradodot.info/staticdata/Statistics/dsp_folder/Demographic/PopCoProj30.htm

```{r, warning = FALSE, message = FALSE}
countyPopulation_2029 <- 
  data.frame(
   NAME = 
     c("Adams","Arapahoe","Broomfield", "Clear Creek", "Denver","Douglas", "Elbert", "Gilpin", "Jefferson", "Park"),
   county_projection_2029 = 
     c(693540,666262,71984,14735,753720,439585,58759,7458,709958,85557)) %>%
   left_join(
     dat %>%
       st_set_geometry(NULL) %>%
       group_by(NAME) %>%
       summarize(county_population_2019 = round(sum(pop_2019))))

countyPopulation_2029 %>%
  gather(Variable,Value, -NAME) %>%
  ggplot(aes(reorder(NAME,-Value),Value)) +
  geom_bar(aes(fill=Variable), stat = "identity", position = "dodge") +
  scale_fill_manual(values = palette2,
                    labels=c("2029","2019"),
                    name="Population") +
  labs(title="Population Change by County: 2019 - 2029",
       x="County", y="Population") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  plotTheme
```

Next, the `countyPopulation_2029` table is joined to `dat` and `pop_change` in order to 'distribute' the new population across the study area. To do so, the the allocation of new population is weighted by a grid cell's existing population (`pop_2029.infill`). 2019 population is subtracted from this figure to get `pop_Change`. Finally, `Model4` is used to predict for 2029 given the updated population change and lag development features.

The map of predicted probabilities that results is best thought of as a measure of predicted development demand in 2029.

```{r, warning = FALSE, message = FALSE}
dat_infill <-
  dat %>%
  #calculate population change
    left_join(countyPopulation_2029) %>%
    mutate(proportion_of_county_pop = pop_2019 / county_population_2019,
           pop_2029.infill = proportion_of_county_pop * county_projection_2029,
           pop_Change = round(pop_2029.infill - pop_2019),2) %>%
    dplyr::select(-county_projection_2029, -county_population_2019, 
                  -proportion_of_county_pop, -pop_2029.infill) %>%
  #predict for 2029
    mutate(predict_2029.infill = predict(Model4,. , type="response"))

dat_infill %>%
  ggplot() +  
  geom_point(aes(x=xyC(dat_infill)[,1], y=xyC(dat_infill)[,2], colour = factor(ntile(predict_2029.infill,5)))) +
  scale_colour_manual(values = palette5,
                    labels=substr(quintileBreaks(dat_infill,"predict_2029.infill"),1,4),
                    name="Quintile\nBreaks") +
  geom_sf(data=studyAreaCounties, fill=NA, colour="black", size=1) +
  labs(title= "Development Demand in 2029: Predicted Probabilities") +
  mapTheme
```

# 3. Comparing Predicted Development Demand & Environmental Sensitivity

We now have a really strong indicator of development demand for 2029 to help guide local land use planning. Demand however, is only one side of the equation. It must balanced with the supply of environmentally sensitive land. Understanding the interplay between demand and supply is the first stage of the 'Allocation' phase, where Planners ultimately decide which land should be developed and which should not.

For this analysis farmland and undeveloped land are be deemed `Suitable`, while environmentally sensitive areas like wetlands and forest are be deemed `Not Suitable`. Below, 2019 land cover data is read in and several measures of environmental sensitivity are created by county. These include:

1.  The total amount of wetlands and forest land cover area in 2019.
2.  The amount of sensitive land (wetland and forest) lost between 2009 and 2019.
3.  The total area of large sensitive landscape 'patches' in 2019.

## 3.1. 2019 Land Cover Data

To begin, the 2019 Land Cover data is read in and reclassified.

```{r, warning = FALSE, message = FALSE}

developed19 <- lc_2019 == 21 | lc_2019 == 22 | lc_2019 == 23 | lc_2019 == 24
forest19 <- lc_2019 == 41 | lc_2019 == 42 | lc_2019 == 43 
farm19 <- lc_2019 == 81 | lc_2019 == 82 
wetlands19 <- lc_2019 == 90 | lc_2019 == 95 
otherUndeveloped19 <- lc_2019 == 52 | lc_2019 == 71 | lc_2019 == 31 
water19 <- lc_2019 == 11

names(developed19) <- "developed19"
names(forest19) <- "forest19"
names(farm19) <- "farm19"
names(wetlands19) <- "wetlands19"
names(otherUndeveloped19) <- "otherUndeveloped19"
names(water19) <- "water19"

ggplot() +
  geom_sf(data=denverMSA) +
  geom_raster(data = rbind(rast(lc_2009) %>% mutate(label = "2009"),
                           rast(lc_2019) %>% mutate(label = "2019")) %>% 
              na.omit %>% filter(value > 0), 
              aes(x,y,fill=as.factor(value))) +
  facet_wrap(~label) +
  scale_fill_viridis(discrete=TRUE, name ="") +
  labs(title = "Land Cover, 2009 & 2019") +
  mapTheme + theme(legend.position = "none")
```

Next, each raster is aggregated to the fishnet using the `aggregateRaster` function and 2019 land cover types are mapped.

```{r, warning = FALSE, message = FALSE}
theRasterList19 <- c(developed19,forest19,farm19,wetlands19,otherUndeveloped19,water19)

dat2 <-
  aggregateRaster(theRasterList19, dat) %>%
  dplyr::select(developed19,forest19,farm19,wetlands19,otherUndeveloped19,water19) %>%
  st_set_geometry(NULL) %>%
  bind_cols(.,dat) %>%
  st_sf() %>%
  st_cast("POLYGON")

dat2 %>%
  gather(var,value,developed19:water19) %>%
  st_centroid() %>%
  mutate(X = st_coordinates(.)[,1],
         Y = st_coordinates(.)[,2]) %>%
  ggplot() +
    geom_sf(data=denverMSA) +
    geom_point(aes(X,Y, colour=as.factor(value))) +
    facet_wrap(~var) +
    scale_colour_manual(values = palette2,
                        labels=c("Other","Land Cover"),
                        name = "") +
    labs(title = "Land Cover Types, 2019",
         subtitle = "As fishnet centroids") +
   mapTheme
```

## 3.2. Sensitive Land Cover Lost

Below an indicator `sensitive_lost` is created indicating grid cells that were either forest or wetlands in 2009 but were no longer so in 2019. The output layer, `sensitive_land_lost`, gives a sense for how development in the recent past has effected the natural environment.

```{r, warning = FALSE, message = FALSE}
dat2 <-
  dat2 %>%
   mutate(sensitive_lost19 = ifelse(forest == 1 & forest19 == 0 |
                                    wetlands == 1 & wetlands19 == 0,1,0))
                      
ggplot() +
  geom_point(data=dat2, aes(x=xyC(dat2)[,1], y=xyC(dat2)[,2], colour=as.factor(sensitive_lost19))) +
  scale_colour_manual(values = palette2,
                      labels=c("No Change","Sensitive Lost"),
                      name = "") +
  labs(title = "Sensitive lands lost: 2009 - 2019",
       subtitle = "As fishnet centroids") +
  mapTheme
```

## 3.3  Landscape Fragmentation

In this section, the `wetlands11` and `forest11` rasters are converted to contiguous `sensitive_regions` using the `raster::clump` function. This is equivalent to Region Group in ArcGIS. The raster clumps are then converted to vector `sf` layers; dissolved into unique regions; Acres are calculated; and the layers are converted back to raster to be extracted back to the fishnet with `aggregateRaster`. It is worth going through this code block line by line. Note that only `sensitive_regions` with areas greater than 1 acre are included.

```{r, warning = FALSE, message = FALSE, fig.height = 4, fig.width= 6}
# Create a raster layer with the same extent and resolution as the other raster layers
emptyRaster[] <- NA

sensitiveRegions <- 
  raster::clump(wetlands19 + forest19) %>%
  rasterToPolygons() %>%
  st_as_sf() %>%
  group_by(clumps) %>% 
  summarize() %>%
    mutate(Acres = as.numeric(st_area(.) * 0.0000229568)) %>%
    filter(Acres > 3954)  %>%
  dplyr::select() %>%
  raster::rasterize(.,emptyRaster) 
sensitiveRegions[sensitiveRegions > 0] <- 1  
names(sensitiveRegions) <- "sensitiveRegions"

dat2 <- 
  aggregateRaster(c(sensitiveRegions), dat2) %>%
  rename(region_type_2 = sensitiveRegions) %>%
  st_set_geometry(NULL) %>%
  bind_cols(.,dat2) %>%
  st_sf()

ggplot() +
  geom_point(data=dat2, aes(x=xyC(dat2)[,1], y=xyC(dat2)[,2], colour=as.factor(region_type_2))) +
  scale_colour_manual(values = palette2,
                      labels=c("Other","Sensitive Regions"),
                      name="") +
  labs(title = "Sensitive regions",
       subtitle = "Continous areas of either wetlands or forests\ngreater than 1 acre") +
  mapTheme

```

## 3.4. Summarize by County

The below `dplyr` statement takes as its input, `dat2`, which was created in Sections 6.2 - 6.4 and wrangles together a table of county-level, supply and demand metrics which can be used to analyze suitability by county.

```{r, warning = FALSE, message = FALSE}
library(raster)

county_specific_metrics <- 
  dat2 %>%
  # extract values of sensitiveRegions at county centroids
  mutate(Sensitive_Regions = raster::extract(sensitiveRegions, sf::st_centroid(dat), fun = sum)) %>%
  #predict development demand from our model
  mutate(Development_Demand = predict(Model4, dat2, type="response")) %>%
  #get a count count of grid cells by county which we can use to calculate rates below
  left_join(st_set_geometry(dat, NULL) %>% group_by(NAME) %>% summarize(count = n())) %>%
  #calculate summary statistics by county
  group_by(NAME) %>%
  summarize(Total_Farmland = sum(farm19) / max(count),
            Total_Forest = sum(forest19) / max(count),
            Total_Wetlands = sum(wetlands19) / max(count),
            Total_Undeveloped = sum(otherUndeveloped19) / max(count),
            Sensitive_Land_Lost = sum(sensitive_lost19) / max(count),
            Sensitive_Regions = sum(Sensitive_Regions) / max(count),
            Mean_Development_Demand = mean(Development_Demand)) %>%
  #get population data by county
  left_join(countyPopulation_2029 %>% 
            mutate(Population_Change = county_projection_2029 - county_population_2019,
                   Population_Change_Rate = Population_Change / county_projection_2029) %>%
            dplyr::select(NAME,Population_Change_Rate))

colnames(county_specific_metrics)

```

Now a small multiple plot can be created providing both supply and demand side analytics by county. The plot gives a sense for development demand (`Demand-Side`), suitable land for development (`Suitable`) and sensitive land (`Not Suitable`).

In Fort Bend County, an area west of denver, the data suggests both population and development demand will increase. At the same time, there is a high rate of developable farmland and a low supply of sensitive land. Fort Bend is well suitable to new development.

Conversely, San Jacinto, the county north and east of denver, contains some of the highest rates of sensitive land use in the region. There is a very large National Forest in this area. In most counties, there are some real trade-offs to be made between suitable/sensitive land and development pressure.

```{r, warning = FALSE, message = FALSE}
county_specific_metrics %>%
  gather(Variable, Value, -NAME, -geometry...2) %>%
  mutate(Variable = factor(Variable, levels=c("Population_Change_Rate","Mean_Development_Demand",
                                              "Total_Farmland","Total_Undeveloped","Total_Forest",
                                              "Total_Wetlands","Sensitive_Land_Lost","Sensitive_Regions",
                                              ordered = TRUE))) %>%
  mutate(Planning_Designation = case_when(
    Variable == "Population_Change_Rate" | Variable == "Mean_Development_Demand" ~ "Demand-Side",
    Variable == "Total_Farmland" | Variable == "Total_Undeveloped"               ~ "Suitable",
    TRUE                                                                         ~ "Not Suitable")) %>%
  ggplot(aes(x=Variable, y=Value, fill=Planning_Designation)) +
    geom_bar(stat="identity", position=position_dodge(), colour="black") +
    facet_wrap(~NAME, ncol=5) +
    coord_flip() +
    scale_y_continuous(breaks = seq(.25, 1, by = .25)) +
    geom_vline(xintercept = 2.5) + geom_vline(xintercept = 4.5) +
    scale_fill_manual(values=c("black","red","darkgreen")) +
    labs(title= "County Specific Allocation Metrics", subtitle= "As rates", x="Indicator", y="Rate") +
    plotTheme + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position="bottom")
```
